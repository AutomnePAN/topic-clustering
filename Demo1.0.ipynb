{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Package Import\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Input, Flatten\n",
    "from keras import Model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "from topic_clustering_models import Modeling\n",
    "from sub_intents_detection import DataPreprocessing, SubIntentsModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting bot based on Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = \"color:red\">Important!</b>\n",
    "\n",
    "**This demo could only be run with Internet Connetion. Because our dataset and part of the model is online**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset url https://github.com/google-research-datasets/Taskmaster/raw/master/TM-1-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = [\"Haolin Pan\", \"Riade Benbaki\"]\n",
    "__version__ = \"École Polytechnique, 2020/2/21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "__data__ = \"https://github.com/google-research-datasets/Taskmaster/raw/master/TM-1-2019/self-dialogs.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Automne\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Automne\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Automne\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Automne\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "s = Modeling()\n",
    "s.dataPreprocessing()\n",
    "s.modelBuildingAndTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we try to detect an opening phrase of one scenario, one conversation; \n",
    "\n",
    "Normally, this would be a phrase to express a need. For example, <i>\" I want a uber to the airport \"</i>\n",
    "\n",
    "Otherwise if the phrase has no information about the scenario, for expample, <i>\\\" Ok, that's it \\\"</i> we regard this as  non-opening\n",
    "\n",
    "Then between two opening phrases, we have a more stable scenario which allow us to make detection more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The precision based on a 0.2 percentage validation set is up to 93%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the models of topic clustering: \n",
      "================================================================\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node lambda_1/module_apply_default/bilm/CNN_2/Conv2D_6}}]]\n\t [[{{node dense_1/Softmax}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-02a768420976>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[1;34m\"I need a ride from home\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"I want to order something to eat\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"can you activate\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 [\"I want a table in center city\"], [\"Ok that's it!\"]]\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_phrases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The predicted topic of \\\"{} \\\" is : {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_phrases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\source\\repos\\python_jupiter_programs\\新建文件夹\\topic_clustering_models.py\u001b[0m in \u001b[0;36mprediction\u001b[1;34m(self, phrs)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node lambda_1/module_apply_default/bilm/CNN_2/Conv2D_6}}]]\n\t [[{{node dense_1/Softmax}}]]"
     ]
    }
   ],
   "source": [
    "print(\"Test the models of topic clustering: \")\n",
    "print(64 * \"=\")\n",
    "test_phrases = [[\"Where is the nearest Starbucks ?\"],[\"i need to repair my car\"],\\\n",
    "                [\"I need a ride from home\"],[\"I want to order something to eat\"],[\"can you activate\"],\\\n",
    "                [\"I want a table in center city\"], [\"Ok that's it!\"]]\n",
    "res = s.prediction(test_phrases)\n",
    "for i in range(len(res)):\n",
    "    print(\"The predicted topic of \\\"{} \\\" is : {}\".format(test_phrases[i][0], res[i]))\n",
    "print(64 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subintents Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "dp = DataPreprocessing()\n",
    "i2phr = dp.intents_clustering()\n",
    "SIM = SubIntentsModeling(i2phr)\n",
    "if not SIM.trained():\n",
    "    SIM.training_models(\"restaurant\")\n",
    "    SIM.training_models(\"movie\")\n",
    "    SIM.training_models(\"auto\")\n",
    "    SIM.training_models(\"pizza\")\n",
    "    SIM.training_models(\"uber\")\n",
    "    SIM.training_models(\"coffee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After detecting a opening phrase of one topic, we try to detect the intents more specific for the topic\n",
    "\n",
    "For example, in the topic of restaurant reservation, we would like to get the informations like <i>time </i>, <i>location</i> etc.\n",
    "\n",
    "Luckily, the entities which reveals this information is labeled in our data set. \n",
    "\n",
    "For example, in the phrase, <i>Thank you, 8 pm will be fine.</i>, <i>8 pm</i> is marked out as the time information\n",
    "\n",
    "In our model, we try to detect the if the phrase has some information we need, if yes, what type is it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The precision of this model is about 0.7 to 0.8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the models of Sub intent detection: \n",
      "================================================================\n",
      "The predicted intent of \"Where is the nearest restaurant?\" is location\n",
      "The predicted intent of \"This Evening\" is time\n",
      "The predicted intent of \"I want this\" is others\n",
      "The predicted intent of \"We have totally 4\" is num\n",
      "The predicted intent of \"My dad, mom, me and my sister\" is num\n",
      "The predicted intent of \"Do you have some recommends\" is type\n",
      "The predicted intent of \"Thank you! Bye\" is name\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Test the models of Sub intent detection: \")\n",
    "print(64 * \"=\")\n",
    "test_phrases = np.array([[\"Where is the nearest restaurant?\"], [\"This Evening\"], [\"I want this\"],\n",
    "                  [\"We have totally 4\"], [\"My dad, mom, me and my sister\"], [\"Do you have some recommends\"],\n",
    "                  [\"Thank you! Bye\"]])\n",
    "res = SIM.prediction(\"restaurant\", test_phrases)\n",
    "for i in range(len(res)):\n",
    "    print(\"The predicted intent of \\\"{}\\\" is {}\".format(test_phrases[i][0], res[i]))\n",
    "print(64 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Obtaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic allowed: 'auto repair', 'coffee ordering', 'movie booking', 'pizza ordering','restaurant reservation','uber ordering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demo_Conversation(object):\n",
    "    def __init__(self, model_cl, models_si):\n",
    "        self.model_cl = model_cl\n",
    "        self.models_si = models_si\n",
    "        self.classes_arr = ['auto', 'coffee', 'movie', 'non-opening', 'pizza','restaurant','uber']\n",
    "        self.opening_response = {\"auto\" : \"\\tIt seems you want to repair your vehicle\", \\\n",
    "                            \"coffee\" : \"\\tIt seems you want to order some coffee\", \\\n",
    "                            \"movie\" : \"\\tIt seems you want to book movie tickets\", \\\n",
    "                            \"pizza\" : \"\\tIt seems you want to order some pizza\",\\\n",
    "                            \"restaurant\": \"\\tIt seems you want to book a table\",\\\n",
    "                            \"uber\": \"\\tIt seems you want to take ride\"}\n",
    "\n",
    "    def inConversationDetected(self):\n",
    "        \n",
    "        print(\"Conversation Test\")\n",
    "        print(\"Still working on it, not completed\")\n",
    "        print(\"Input \\'~\\' to stop\")\n",
    "        print(64*\"=\")\n",
    "        print(\"\\tHello, what can I do for you?\")\n",
    "        print(64 * \"-\")\n",
    "        userInput = \"\"\n",
    "        current_topic = \"non-opening\";\n",
    "        userInput = input()\n",
    "        \n",
    "        while current_topic == \"non-opening\":\n",
    "            prediction = self.model_cl.predict(np.array([[userInput], [userInput]]))\n",
    "            class_pred = self.classes_arr[np.argmax(prediction[0],axis=0)]\n",
    "            current_topic = class_pred\n",
    "            \n",
    "        print(\"\\ttopic predicted: \", class_pred)\n",
    "        print(\"\\tentering the scenario \", class_pred)\n",
    "        if class_pred not in self.classes_arr:\n",
    "            print(\"Error, there is no class\")\n",
    "            return -1\n",
    "        print(self.opening_response[class_pred])\n",
    "\n",
    "        \n",
    "        while (userInput != \"~\"):  \n",
    "            subintent = self.models_si.prediction(current_topic, np.array([[userInput],[userInput]]))[0]\n",
    "            print(\"\\tIt seems that you want to say something about: \", subintent)    \n",
    "            print(64* \"-\")\n",
    "            userInput = input()\n",
    "        \n",
    "        print(\"Goodbye, thank you for your attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = Demo_Conversation(s.model_lstm, SIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Test\n",
      "Still working on it, not completed\n",
      "Input '~' to stop\n",
      "================================================================\n",
      "\tHello, what can I do for you?\n",
      "----------------------------------------------------------------\n",
      "I want to see some newest movies\n",
      "\ttopic predicted:  movie\n",
      "\tentering the scenario  movie\n",
      "\tIt seems you want to book movie tickets\n",
      "\tIt seems that you want to say something about:  name\n",
      "----------------------------------------------------------------\n",
      "this evening\n",
      "\tIt seems that you want to say something about:  time\n",
      "----------------------------------------------------------------\n",
      "Jusr in Paris\n",
      "\tIt seems that you want to say something about:  location\n",
      "----------------------------------------------------------------\n",
      "We have 4 persons totally, my parents , me and my sister\n",
      "\tIt seems that you want to say something about:  num\n",
      "----------------------------------------------------------------\n",
      "Wonder woman 2 would be great\n",
      "\tIt seems that you want to say something about:  name\n",
      "----------------------------------------------------------------\n",
      "That's it! book it for me\n",
      "\tIt seems that you want to say something about:  name\n",
      "----------------------------------------------------------------\n",
      "~\n",
      "Goodbye, thank you for your attention\n"
     ]
    }
   ],
   "source": [
    "dc.inConversationDetected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Test\n",
      "Still working on it, not completed\n",
      "Input '~' to stop\n",
      "================================================================\n",
      "\tHello, what can I do for you?\n",
      "----------------------------------------------------------------\n",
      "I think my car is broken\n",
      "\ttopic predicted:  auto\n",
      "\tentering the scenario  auto\n",
      "\tIt seems you want to repair your vehicle\n",
      "\tIt seems that you want to say something about:  reason\n",
      "----------------------------------------------------------------\n",
      "There was a little accident\n",
      "\tIt seems that you want to say something about:  reason\n",
      "----------------------------------------------------------------\n",
      "It's a old car, it is 20 years old\n",
      "\tIt seems that you want to say something about:  name\n",
      "----------------------------------------------------------------\n",
      "It's a BMW\n",
      "\tIt seems that you want to say something about:  name\n",
      "----------------------------------------------------------------\n",
      "Could you order repair work next week\n",
      "\tIt seems that you want to say something about:  date\n",
      "----------------------------------------------------------------\n",
      "That's too kind\n",
      "\tIt seems that you want to say something about:  reason\n",
      "----------------------------------------------------------------\n",
      "~\n",
      "Goodbye, thank you for your attention\n"
     ]
    }
   ],
   "source": [
    "dc.inConversationDetected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Test\n",
      "Still working on it, not completed\n",
      "Input '~' to stop\n",
      "================================================================\n",
      "\tHello, what can I do for you?\n",
      "----------------------------------------------------------------\n",
      "I want to go to airport\n",
      "\ttopic predicted:  uber\n",
      "\tentering the scenario  uber\n",
      "\tIt seems you want to take ride\n",
      "\tIt seems that you want to say something about:  location\n",
      "----------------------------------------------------------------\n",
      "As soon as possible\n",
      "\tIt seems that you want to say something about:  time\n",
      "----------------------------------------------------------------\n",
      "We have 5 persons\n",
      "\tIt seems that you want to say something about:  num\n",
      "----------------------------------------------------------------\n",
      "My father, my mother, me and my sister\n",
      "\tIt seems that you want to say something about:  num\n",
      "----------------------------------------------------------------\n",
      "How long could it tak\n",
      "\tIt seems that you want to say something about:  type\n",
      "----------------------------------------------------------------\n",
      "How long could it takes\n",
      "\tIt seems that you want to say something about:  type\n",
      "----------------------------------------------------------------\n",
      "thanks!\n",
      "\tIt seems that you want to say something about:  location\n",
      "----------------------------------------------------------------\n",
      "~\n",
      "Goodbye, thank you for your attention\n"
     ]
    }
   ],
   "source": [
    "dc.inConversationDetected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Test\n",
      "Still working on it, not completed\n",
      "Input '~' to stop\n",
      "================================================================\n",
      "\tHello, what can I do for you?\n",
      "----------------------------------------------------------------\n",
      "Book a restaurant for me\n",
      "\ttopic predicted:  restaurant\n",
      "\tentering the scenario  restaurant\n",
      "\tIt seems you want to book a table\n",
      "\tIt seems that you want to say something about:  num\n",
      "----------------------------------------------------------------\n",
      "Two people, my wife and me\n",
      "\tIt seems that you want to say something about:  num\n",
      "----------------------------------------------------------------\n",
      "Just around the city center\n",
      "\tIt seems that you want to say something about:  location\n",
      "----------------------------------------------------------------\n",
      "I want some real good sea food\n",
      "\tIt seems that you want to say something about:  others\n",
      "----------------------------------------------------------------\n",
      "this evening at 20:00\n",
      "\tIt seems that you want to say something about:  time\n",
      "----------------------------------------------------------------\n",
      "~\n",
      "Goodbye, thank you for your attention\n"
     ]
    }
   ],
   "source": [
    "dc.inConversationDetected()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
